{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T19:35:11.436591Z",
     "iopub.status.busy": "2024-11-13T19:35:11.436178Z",
     "iopub.status.idle": "2024-11-13T19:35:11.441964Z",
     "shell.execute_reply": "2024-11-13T19:35:11.440711Z",
     "shell.execute_reply.started": "2024-11-13T19:35:11.436543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:35:11.666983Z",
     "iopub.status.busy": "2024-11-13T19:35:11.666124Z",
     "iopub.status.idle": "2024-11-13T19:35:11.681341Z",
     "shell.execute_reply": "2024-11-13T19:35:11.680279Z",
     "shell.execute_reply.started": "2024-11-13T19:35:11.666939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pdf-db/tsla-20231231-gen.pdf\n",
      "/kaggle/input/pdf-db/goog-10-k-2023.pdf\n",
      "/kaggle/input/pdf-db/uber-10-k-2023.pdf\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:35:11.898695Z",
     "iopub.status.busy": "2024-11-13T19:35:11.897855Z",
     "iopub.status.idle": "2024-11-13T19:35:12.988084Z",
     "shell.execute_reply": "2024-11-13T19:35:12.986917Z",
     "shell.execute_reply.started": "2024-11-13T19:35:11.898652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 13 19:35:12 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   36C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:35:12.990565Z",
     "iopub.status.busy": "2024-11-13T19:35:12.990165Z",
     "iopub.status.idle": "2024-11-13T19:36:07.403994Z",
     "shell.execute_reply": "2024-11-13T19:36:07.402733Z",
     "shell.execute_reply.started": "2024-11-13T19:35:12.990523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq torch --progress-bar off\n",
    "!pip install -qqq transformers --progress-bar off\n",
    "!pip install -qqq langchain --progress-bar off\n",
    "!pip install -qqq chromadb --progress-bar off\n",
    "!pip install -qqq pypdf --progress-bar off\n",
    "!pip install -qqq xformers --progress-bar off\n",
    "!pip install -qqq sentence_transformers --progress-bar off\n",
    "!pip install -qqq InstructorEmbedding --progress-bar off\n",
    "!pip install -qqq pdf2image --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:36:50.533160Z",
     "iopub.status.busy": "2024-11-13T19:36:50.532748Z",
     "iopub.status.idle": "2024-11-13T19:36:52.079619Z",
     "shell.execute_reply": "2024-11-13T19:36:52.078369Z",
     "shell.execute_reply.started": "2024-11-13T19:36:50.533116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:37:06.596018Z",
     "iopub.status.busy": "2024-11-13T19:37:06.595610Z",
     "iopub.status.idle": "2024-11-13T19:37:12.859280Z",
     "shell.execute_reply": "2024-11-13T19:37:12.857988Z",
     "shell.execute_reply.started": "2024-11-13T19:37:06.595977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:40:18.047618Z",
     "iopub.status.busy": "2024-11-13T19:40:18.047158Z",
     "iopub.status.idle": "2024-11-13T19:40:29.846782Z",
     "shell.execute_reply": "2024-11-13T19:40:29.845822Z",
     "shell.execute_reply.started": "2024-11-13T19:40:18.047566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libpoppler118 poppler-data\n",
      "Suggested packages:\n",
      "  ghostscript fonts-japanese-mincho | fonts-ipafont-mincho\n",
      "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
      "  fonts-arphic-uming fonts-nanum\n",
      "The following NEW packages will be installed:\n",
      "  libpoppler118 poppler-data poppler-utils\n",
      "0 upgraded, 3 newly installed, 0 to remove and 68 not upgraded.\n",
      "Need to get 3427 kB of archives.\n",
      "After this operation, 17.7 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2171 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.5 [1071 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
      "Fetched 3427 kB in 1s (2697 kB/s)       \n",
      "Selecting previously unselected package poppler-data.\n",
      "(Reading database ... 122996 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-data_0.4.11-1_all.deb ...\n",
      "Unpacking poppler-data (0.4.11-1) ...\n",
      "Selecting previously unselected package libpoppler118:amd64.\n",
      "Preparing to unpack .../libpoppler118_22.02.0-2ubuntu0.5_amd64.deb ...\n",
      "Unpacking libpoppler118:amd64 (22.02.0-2ubuntu0.5) ...\n",
      "Selecting previously unselected package poppler-utils.\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
      "Setting up poppler-data (0.4.11-1) ...\n",
      "Setting up libpoppler118:amd64 (22.02.0-2ubuntu0.5) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install poppler-utils -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:40:34.138354Z",
     "iopub.status.busy": "2024-11-13T19:40:34.137391Z",
     "iopub.status.idle": "2024-11-13T19:41:03.741246Z",
     "shell.execute_reply": "2024-11-13T19:41:03.740343Z",
     "shell.execute_reply.started": "2024-11-13T19:40:34.138312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:41:40.694943Z",
     "iopub.status.busy": "2024-11-13T19:41:40.694124Z",
     "iopub.status.idle": "2024-11-13T19:43:03.459364Z",
     "shell.execute_reply": "2024-11-13T19:43:03.458503Z",
     "shell.execute_reply.started": "2024-11-13T19:41:40.694901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pdf_directory = \"/kaggle/input/pdf-db\"\n",
    "loader = PyPDFDirectoryLoader(pdf_directory)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:43:10.521768Z",
     "iopub.status.busy": "2024-11-13T19:43:10.521337Z",
     "iopub.status.idle": "2024-11-13T19:43:24.256780Z",
     "shell.execute_reply": "2024-11-13T19:43:24.255714Z",
     "shell.execute_reply.started": "2024-11-13T19:43:10.521726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1e1dc0ac45484bb87e7710dd1ada76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea7eef7314d4562b39e439578a87bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1aea53caf64dad9bbeda0864fe1351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e47227f8e349978fe062ecff84a32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237b2d00e73344d3b37ee118e3ffb7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd394d60399146a09435b683e9b6d32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d16355b500845c3a3a3660d419c4925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941bbf1fec1d4c20ab2c5803b0d6f99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4dee3ce2a64bb1a82e80b6b293b70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f906195293a43dcb86301ef6979d5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9442394daab944a59033792a65edb0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba601c4107b411490e65de5fac5c63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecffd0e618364b6380fe42e4c8164935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db884ee11a3c4ed48db0d566b82c2cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:43:40.240540Z",
     "iopub.status.busy": "2024-11-13T19:43:40.239844Z",
     "iopub.status.idle": "2024-11-13T19:43:40.344532Z",
     "shell.execute_reply": "2024-11-13T19:43:40.343408Z",
     "shell.execute_reply.started": "2024-11-13T19:43:40.240489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1713"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:44:24.074736Z",
     "iopub.status.busy": "2024-11-13T19:44:24.073774Z",
     "iopub.status.idle": "2024-11-13T19:46:19.367122Z",
     "shell.execute_reply": "2024-11-13T19:46:19.366261Z",
     "shell.execute_reply.started": "2024-11-13T19:44:24.074692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Chroma database at: /kaggle/working/db\n"
     ]
    }
   ],
   "source": [
    "persist_directory = \"/kaggle/working/db\"\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    print(\"Creating Chroma database at:\", persist_directory)\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory)\n",
    "else:\n",
    "    print(\"Loading existing Chroma database from:\", persist_directory)\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:53:40.467946Z",
     "iopub.status.busy": "2024-11-13T19:53:40.467548Z",
     "iopub.status.idle": "2024-11-13T19:54:19.371517Z",
     "shell.execute_reply": "2024-11-13T19:54:19.370698Z",
     "shell.execute_reply.started": "2024-11-13T19:53:40.467911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6aad1beded54be4b6be6d78a10e4615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1523 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "# Define device map for GPU setup (if using multiple GPUs, adjust accordingly)\n",
    "device_map = \"auto\"  # This automatically assigns devices in a multi-GPU setup. Use \"cuda:0\" for a specific GPU.\n",
    "\n",
    "# Load the model\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    revision=\"gptq-4bit-128g-actorder_True\",\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    inject_fused_attention=False,\n",
    "    device=DEVICE,  # Ensure device is passed\n",
    "    device_map=device_map,  # This will allow automatic device assignment\n",
    "    quantize_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:54:39.626969Z",
     "iopub.status.busy": "2024-11-13T19:54:39.626083Z",
     "iopub.status.idle": "2024-11-13T19:54:39.632193Z",
     "shell.execute_reply": "2024-11-13T19:54:39.631232Z",
     "shell.execute_reply.started": "2024-11-13T19:54:39.626927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <>\n",
    "{system_prompt}\n",
    "<>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:54:53.704554Z",
     "iopub.status.busy": "2024-11-13T19:54:53.703689Z",
     "iopub.status.idle": "2024-11-13T19:54:53.708688Z",
     "shell.execute_reply": "2024-11-13T19:54:53.707672Z",
     "shell.execute_reply.started": "2024-11-13T19:54:53.704512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:05.408600Z",
     "iopub.status.busy": "2024-11-13T19:55:05.407874Z",
     "iopub.status.idle": "2024-11-13T19:55:06.542704Z",
     "shell.execute_reply": "2024-11-13T19:55:06.541727Z",
     "shell.execute_reply.started": "2024-11-13T19:55:05.408555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:19.731322Z",
     "iopub.status.busy": "2024-11-13T19:55:19.730912Z",
     "iopub.status.idle": "2024-11-13T19:55:19.771170Z",
     "shell.execute_reply": "2024-11-13T19:55:19.770051Z",
     "shell.execute_reply.started": "2024-11-13T19:55:19.731280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:33.571800Z",
     "iopub.status.busy": "2024-11-13T19:55:33.571156Z",
     "iopub.status.idle": "2024-11-13T19:55:33.576273Z",
     "shell.execute_reply": "2024-11-13T19:55:33.575329Z",
     "shell.execute_reply.started": "2024-11-13T19:55:33.571756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:45.896260Z",
     "iopub.status.busy": "2024-11-13T19:55:45.895619Z",
     "iopub.status.idle": "2024-11-13T19:55:45.907366Z",
     "shell.execute_reply": "2024-11-13T19:55:45.906401Z",
     "shell.execute_reply.started": "2024-11-13T19:55:45.896220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:56:00.835743Z",
     "iopub.status.busy": "2024-11-13T19:56:00.835343Z",
     "iopub.status.idle": "2024-11-13T19:56:00.847729Z",
     "shell.execute_reply": "2024-11-13T19:56:00.846692Z",
     "shell.execute_reply.started": "2024-11-13T19:56:00.835707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:56:59.478704Z",
     "iopub.status.busy": "2024-11-13T19:56:59.477475Z",
     "iopub.status.idle": "2024-11-13T19:57:40.718308Z",
     "shell.execute_reply": "2024-11-13T19:57:40.717373Z",
     "shell.execute_reply.started": "2024-11-13T19:56:59.478657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I can provide information on the risk factors associated with Tesla, as described in the provided text. However, I cannot provide information on the risk factors associated with Google, as there is no mention of Google in the text.\n",
      "\n",
      "According to the text, the risk factors associated with Tesla include:\n",
      "\n",
      "1. Competition for skilled individuals with automotive or technology experience, particularly in regions where Tesla has or will have operations.\n",
      "2. Strong competition from mature and prosperous companies with far greater financial resources than Tesla, as well as start-ups and emerging companies that promise short-term growth opportunities.\n",
      "3. The emphasis on equity-based awards and benefits in Tesla's compensation philosophy, which reflects its startup origins.\n",
      "4. Negative publicity related to Tesla, which may cause employees to leave the company or choose other employers over Tesla.\n",
      "5. Various factors that may harm Tesla's business, financial condition, and operating results, such as a very competitive labor market for talented individuals with automotive or technology experience.\n",
      "{'query': 'What are the risk factors associated with Google and Tesla?', 'result': \"  I can provide information on the risk factors associated with Tesla, as described in the provided text. However, I cannot provide information on the risk factors associated with Google, as there is no mention of Google in the text.\\n\\nAccording to the text, the risk factors associated with Tesla include:\\n\\n1. Competition for skilled individuals with automotive or technology experience, particularly in regions where Tesla has or will have operations.\\n2. Strong competition from mature and prosperous companies with far greater financial resources than Tesla, as well as start-ups and emerging companies that promise short-term growth opportunities.\\n3. The emphasis on equity-based awards and benefits in Tesla's compensation philosophy, which reflects its startup origins.\\n4. Negative publicity related to Tesla, which may cause employees to leave the company or choose other employers over Tesla.\\n5. Various factors that may harm Tesla's business, financial condition, and operating results, such as a very competitive labor market for talented individuals with automotive or technology experience.\", 'source_documents': [Document(page_content='these\\trisks,\\tour\\tbusiness,\\tfinancial\\tcondition\\tand\\toperating\\tresults\\tmay\\tbe\\tharmed.\\nEmployees\\tmay\\tleave\\tTesla\\tor\\tchoose\\tother\\temployers\\tover\\tTesla\\tdue\\tto\\tvarious\\tfactors,\\tsuch\\tas\\ta\\tvery\\tcompetitive\\tlabor\\tmarket\\tfor\\ttalented\\nindividuals\\twith\\tautomotive\\tor\\ttechnology\\texperience,\\tor\\tany\\tnegative\\tpublicity\\trelated\\tto\\tus.\\tIn\\tregions\\twhere\\twe\\thave\\tor\\twill\\thave\\toperations,\\nparticularly\\tsignificant\\tengineering\\tand\\tmanufacturing\\tcenters,\\tthere\\tis\\tstrong\\tcompetition\\tfor\\tindividuals\\twith\\tskillsets\\tneeded\\tfor\\tour\\tbusiness,\\tincluding\\nspecialized\\tknowledge\\tof\\telectric\\tvehicles,\\tengineering\\tand\\telectrical\\tand\\tbuilding\\tconstruction\\texpertise.\\tWe\\talso\\tcompete\\twith\\tboth\\tmature\\tand\\nprosperous\\tcompanies\\tthat\\thave\\tfar\\tgreater\\tfinancial\\tresources\\tthan\\twe\\tdo\\tand\\tstart-ups\\tand\\temerging\\tcompanies\\tthat\\tpromise\\tshort-term\\tgrowth\\nopportunities.\\nFinally,\\tour\\tcompensation\\tphilosophy\\tfor\\tall\\tof\\tour\\tpersonnel\\treflects\\tour\\tstartup\\torigins,\\twith\\tan\\temphasis\\ton\\tequity-based\\tawards\\tand\\tbenefits\\tin', metadata={'page': 20, 'source': '/kaggle/input/pdf-db/tsla-20231231-gen.pdf'}), Document(page_content='www.sec.gov\\n)\\tthat\\tcontains\\nreports,\\tproxy\\tand\\tinformation\\tstatements,\\tand\\tother\\tinformation\\tregarding\\tissuers\\tthat\\tfile\\telectronically.\\tOur\\twebsite\\tis\\tlocated\\tat\\t\\nwww.tesla.com\\n,\\tand\\nour\\treports,\\tamendments\\tthereto,\\tproxy\\tstatements\\tand\\tother\\tinformation\\tare\\talso\\tmade\\tavailable,\\tfree\\tof\\tcharge,\\ton\\tour\\tinvestor\\trelations\\twebsite\\tat\\nir.tesla.com\\n\\tas\\tsoon\\tas\\treasonably\\tpracticable\\tafter\\twe\\telectronically\\tfile\\tor\\tfurnish\\tsuch\\tinformation\\twith\\tthe\\tSEC.\\tThe\\tinformation\\tposted\\ton\\tour\\twebsite\\nis\\tnot\\tincorporated\\tby\\treference\\tinto\\tthis\\tAnnual\\tReport\\ton\\tForm\\t10-K.\\nITEM\\t1A.\\tRISK\\tFACTORS\\nYou\\tshould\\tcarefully\\tconsider\\tthe\\trisks\\tdescribed\\tbelow\\ttogether\\twith\\tthe\\tother\\tinformation\\tset\\tforth\\tin\\tthis\\treport,\\twhich\\tcould\\tmaterially\\taffect\\tour\\nbusiness,\\tfinancial\\tcondition\\tand\\tfuture\\tresults.\\tThe\\trisks\\tdescribed\\tbelow\\tare\\tnot\\tthe\\tonly\\trisks\\tfacing\\tour\\tcompany.\\tRisks\\tand\\tuncertainties\\tnot\\tcurrently', metadata={'page': 14, 'source': '/kaggle/input/pdf-db/tsla-20231231-gen.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain(\"What are the risk factors associated with Google and Tesla?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T20:01:09.727348Z",
     "iopub.status.busy": "2024-11-13T20:01:09.726593Z",
     "iopub.status.idle": "2024-11-13T20:01:41.397680Z",
     "shell.execute_reply": "2024-11-13T20:01:41.396689Z",
     "shell.execute_reply.started": "2024-11-13T20:01:09.727305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the context, the total revenue for Google Search is not explicitly stated. However, we can calculate it based on the information given.\n",
      "\n",
      "In 2021, the revenues from Google Search & other were $148,951 million.\n",
      "\n",
      "In 2022, the revenues from Google Search & other were $162,450 million.\n",
      "\n",
      "In 2023, the revenues from Google Search & other were $175,033 million.\n",
      "\n",
      "Therefore, the total revenue for Google Search from 2021 to 2023 is:\n",
      "\n",
      "$148,951 + $162,450 + $175,033 = $486,434 million.\n",
      "\n",
      "So, the total revenue for Google Search is $486,434 million.\n",
      "  Based on the information provided in the context, the total revenue for Google Search is not explicitly stated. However, we can calculate it based on the information given.\n",
      "\n",
      "In 2021, the revenues from Google Search & other were $148,951 million.\n",
      "\n",
      "In 2022, the revenues from Google Search & other were $162,450 million.\n",
      "\n",
      "In 2023, the revenues from Google Search & other were $175,033 million.\n",
      "\n",
      "Therefore, the total revenue for Google Search from 2021 to 2023 is:\n",
      "\n",
      "$148,951 + $162,450 + $175,033 = $486,434 million.\n",
      "\n",
      "So, the total revenue for Google Search is $486,434 million.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain(\"What is the total revenue for Google Search?\")\n",
    "for k,v in result.items():\n",
    "    if k==\"result\":\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T20:04:26.610519Z",
     "iopub.status.busy": "2024-11-13T20:04:26.610130Z",
     "iopub.status.idle": "2024-11-13T20:04:33.910955Z",
     "shell.execute_reply": "2024-11-13T20:04:33.910008Z",
     "shell.execute_reply.started": "2024-11-13T20:04:26.610482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the context, the main business of Uber is to develop and operate a technology platform that connects consumers with providers of ride services, meal preparation, grocery, and other delivery services, as well as with public transportation networks.\n",
      "  Based on the information provided in the context, the main business of Uber is to develop and operate a technology platform that connects consumers with providers of ride services, meal preparation, grocery, and other delivery services, as well as with public transportation networks.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain(\"What is main business of Uber?\")\n",
    "for k,v in result.items():\n",
    "    if k==\"result\":\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6080223,
     "sourceId": 9898422,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
